---
title: study note of k8s CKA exam
description: 整理udemy裡面,k8s考CKA課程的教學內容
authors: suyuying
tags: [k8s]
draft: true
---
## 指令統整區

```bash
#查詢你的k8s系統狀態
kubectl get pods --namespace kube-system

```

## 基礎架構區

1. API Server：API Server 就像碼頭的中央控制中心，它提供了與外部世界（用戶、開發人員）通信的接口，處理請求並將命令傳達給各個部門（組件）。

2. kube-scheduler：kube-scheduler 就像港口的貨物調度員，負責將貨物（Pod）分配給可用的船只（Worker Node），選擇最適合的船只以確保貨物的安全和高效運輸。

3. Controller Manager：Controller Manager 包括各種控制器，如 Node Controller 和 Replica Set Controller，它們就像港口的管理團隊，負責監控貨物（Node 和 Replica Set）的狀態,例如，Node Controller 管理節點的狀態，Replica Set 監視副本集的狀態，並確保所需的 Pod 數量等。

4. etcd：etcd 就像碼頭的記錄簿，它記錄了所有貨物何時到港裝箱以及放到哪艘船上的信息，是集群配置的存儲後端。

5. Kubelet：Kubelet 就像每艘船上的船長，負責管理貨物（容器）的裝卸，接收並執行由 API Server 下發的命令以創建和管理容器（Pod），同時負責監控容器的狀態並向 API Server 報告,本身也可以透過kubelet指令管理container.

6. Kube Proxy：Kube Proxy 就像連接各艘船的通信管道，確保貨物（Pod）之間可以相互通信，維護網絡規則和負載均衡,負責nodes間的pods與services間的網路溝通.Kube Proxy 使用名稱解析，因此不需要擔心 Pod 的 IP 地址變化。

7. Container Runtime：容器運行時就像每艘船上的船員，負責在船上運行和維護貨物（容器），確保它們按照規定的方式運行。Pod 可以包含一個主容器，以及一個或多個輔助容器。

簡單運作動向：

- 基礎pod建立:
指令-> API server -> kubescheduler -> kublet -> create POD.  貨物(pod)通知中央控制中心(API server),API server會告知 kubeshcduler 尋找適合規格的船隻(node)放貨物, kubeshcduler找到標的後把紀錄寫到etcd,後續kubelet(船長)與apiserver溝通時,會把貨物(pod)調度到node上,並監控貨物狀態.


- 使用deployment:
指令-> API server-> Controller Manager -> kubescheduler-> kublet -> create POD.  貨物(pod)通知中央控制中心(API server),API server透過Controller Manager管理各貨物狀態,以及告知 kubeshcduler 尋找適合規格的船隻(node)放貨物, kubeshcduler找到標的後把紀錄寫到etcd,後續kubelet(船長)與apiserver溝通時,會把貨物(pod)調度到node上,並確保貨物活著,Controller Manager也會做統整式的回報.

## K8s 宣布與 Dockershim 分手

在k8s v1.24之後k8s跟Dockershim分手了,這是不是代表docker不能在k8s用了? 先說結論,一樣可以用！ 主要原因就是docker是早於k8s開發,k8s的容器標準CRI規範相較於docker的container規範不同,為此k8s社群使用dockershim去處理docker的容器邏輯,但之後docker被Mirantis買走了,所以k8s社群就規劃不在維護dockershim了,但後續買走docker的Mirantis也宣布推出cri-dockerd取代dockershim,所以結論是k8s一樣可以使用docker做為底層contaier run time,只是管理docker底層的換人了. [詳細可以參考這篇](https://www.geminiopencloud.com/zh-tw/blog/dockershim-removed/)

## ETCD 基礎篇

1. ETCD 是 Key-Value 存儲： ETCD 是一個分佈式的 Key-Value 存儲系統，用於存儲配置數據和集群狀態。它可以單獨安裝並運行，也是 Kubernetes 集群的核心組件之一。
   
2. 關聯資料庫和 Key-Value 非關聯資料庫存儲的差異： 在關聯式數據庫中，數據以表格形式存儲，並且需要事先定義結構，如果需要新增欄位，可能會對整個表格造成影響。而在 Key-Value 存儲中，數據以鍵值對的形式存儲，每個值都是相互獨立的，不需要預先定義結構，這意味著可以輕鬆添加或刪除鍵值對，而不影響其他數據。

3. ETCD 操作示例： 使用 etcdctl 工具可以對 ETCD 進行操作，例如設置（set）鍵值對、獲取（get）數據等。ETCD 的版本不斷更新，不同版本的 etcdctl 工具可能有不同的指令。
4. CNCF 孵化項目： ETCD 是 Cloud Native Computing Foundation（CNCF）的孵化項目，它於 2018 年成為 CNCF 孵化項目的一部分，這證明了它在雲原生生態系統中的重要性。

## 跟kubeapi溝通的文件規範

基本上要列出的四個規範:

1. apiversion
2. kind
3. metadata
4. spec

底下為一個基礎建立pod的示範！
看到metadata代表是正式的東東,底下name這些都要和規範,label那邊就沒差！

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ford-nginx
  labels:
    app: ford-app
    type: ford-nginx
spec:
  containers:
    - name: nginx
      image: nginx
```

使用replica set建立多個pod

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replicaset
spec:
  replicas: 3  # 指定副本数量
  selector:
    matchLabels:
      app: myapp  # 根据标签选择要管理的 Pod
  template:
    metadata:
      labels:
        app: myapp  # Pod 的标签，与选择器匹配
    spec:
      containers:
        - name: my-container
          image: 幫我排版改大小寫等  # 容器的镜像
```

```bash
kubectl get replicasets.apps new-replica-set -o yaml
kubectl delete replicaset metadata_name_of_replica_set
kubectl scale —replicas=6 -f xxx.yml
kubectl scale —replicas=6 replicaset metadata_name_of_replica_set

```

## deployment

deployment是高replicaset一級的存在,使用他也會一起創照replicaset,主要是哪來執行複雜的update功能.

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      strategy:
        rollingUpdate:
          maxSurge: 25%
          maxUnavailable: 25%
      type: RollingUpdate
      containers:
        - name: busybox-container
          image: busybox888
          command:
            - sh
            - "-c"
            - echo Hello Kubernetes! && sleep 3600

```

## services

服務是 Kubernetes 中用於連接應用程序與用戶或其他應用程序的方式。它們有不同的類型，包括 NodePort、ClusterIP 和 LoadBalancer。

1. NodePort 允許在節點上開放一個端口，將流量轉發到 Pod 的目標端口(要連過來要打node的ip,預設30000~32767),用於對外

```yaml
apiVersion: v1
kind: Service
metadata:
 name: myapp-service
spec:
 types: NodePort
 ports:
 - targetPort: 80
   port: 80
   nodePort: 30008
```

這邊target port是pod的ip,會是weave這類deamon-set網段所規劃出來的ip, port這部分是cluster-ip-range部分,nodeport就是你node的Ip上面開的port.

2. clusterIP 提供集群內部的虛擬 IP，使用服務名稱連接,主要用於內部服務間彼此溝通,用於對內,cluster IP是根據api-server裡面參數定義`--service-cluster-ip-range=10.96.0.0/12`去分配的
3. LoadBalancer 使用外部負載均衡器來公開服務,用於對外

這邊補充一下, pod的ip分配是基於你使用的網路服務就去建立的,ex.我用weave就會是需要在那邊定義.

```bash
controlplane ~ ➜  kubectl get service
NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
blue-service   NodePort   10.43.231.188   <none>        8080:30082/TCP   18m
db-service     NodePort   10.43.135.70    <none>        6379:32194/TCP   18m
```

這邊PORT部分,以db-service為例,6379是port on pod, 32194是port on node,所以服務間要連接用6379

## Namespaces

命名空間 (Namespaces)： 命名空間用於在 Kubernetes 集群中創建虛擬分區，以隔離不同應用程序或用戶的資源。每個命名空間都有自己的資源，例如 Pod、Deployment 等.
連接不同命名空間的服務： 如果需要在不同的命名空間之間建立連接，必須使用完全限定的主機名（FQDN），例如 db-service.dev.svc.cluster.local，以訪問其他命名空間中的服務.

```bash
#查詢方式,不設定-n會用default.
kubectl get pods —n=kube-system
#切換上下文
 kubectl config set-context $(kubectl config current-context) --namespace=marketing
# 所有namespace都查
kubectl get pods —A
```

## k8s 裡面的dry run 原理

k8s跟terraform一樣會有管理狀態的地方,簡單說他會做中繼file先進行狀態比對,就跟terraform state一樣,所以他`--dry-run=client`才跑的成功！在使用指令跟用文檔敘述的部分不能混合使用！not to mixing imperative and decorative！


## label 用法

```bash
kubectl get pods -l bu=finance
kubectl get all --show-labels
kubectl get all -l env=prod
kubectl get all -l bu=finance,env=prod,tier=frontend
kubectl get pods --all-namespaces -o wide
kubectl label nodes node01 color=blue
```

## taint and tolerant

`kubectl describe node kubemaster |grep -i taint` 可以用這個指令看masternode設定的taint,taint功用是限制node接受哪些pod(需要有對應的tolerations),功用包含是區分環境,節點維護,以及減少特定node(ex.controlplane)負擔.也可以給pod設定tolerations,讓他可以在相符有taint的node透過scheduler規劃該pod.

- There are 3 taint effects
  - **`NoSchedule`**
  - **`PreferNoSchedule`**
  - **`NoExecute`**

NoSchedule（不调度）：tolerate不和不會過來，例如用于特殊用途的节点（如master节点）。
PreferNoSchedule（优先不调度）：在这种模式下，Taint 会告诉调度器尽量不要将不匹配 Taint 的 Pod 调度到节点上，但不会阻止它们。只有当没有其他可行的节点时，才会调度这些 Pod 到被 Taint 的节点上。
NoExecute（不执行）：在这种模式下，Taint 不仅会阻止不匹配 Taint 的新 Pod 被调度到节点上，还会在已经运行在节点上的不匹配 Taint 的 Pod 上触发终止行动，以便将它们从节点上删除。

以下是設定node有taint,並在對應pod上設定tolerations.

```yaml
apiVersion: v1
kind: Node
metadata:
  name: mynode
spec:
  taints:
  - key: "app"
    value: "blue"
    effect: "NoSchedule"
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
```

## Node Selectors

功用是讓pod選擇node,Node Selectors相比於node affinity 算是簡易版的pod選node的選擇器！使用情境:如果你有一格很吃資源的pod,會規劃放在資源相對多的node上面.
以下是帶有nodeSelector的pod,他必須要對應到有label的node,`kubectl label nodes node-1 size=Large`.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: data-processor
   image: data-processor
 nodeSelector:
  size: Large
```

## node affinity 複雜情境會用

如果今天pod想要到資源是large or medium的node,或者只想設定not small,要怎麼操作？！

以下是希望pod對應的node的size是large or medium的設定,這邊的requiredDuringSchedulingIgnoredDuringExecution 在調度時會有差,對於已經存在於node上但不符合規則的pod會做相應處理.

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: data-processor
   image: data-processor
 affinity:
   nodeAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values: 
            - Large
            - Medium
```

以下是限定pod不能在node為size是small的label

```yaml
apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: data-processor
   image: data-processor
 affinity:
   nodeAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: NotIn
            values: 
            - Small
```

對應的node基本需要有label設定:

```yaml
apiVersion: v1
kind: Node
metadata:
  name: node-1
  labels:
    size: Large
```

要確保不同團隊的pod不會跑到別家node,先給pod上taint,然後再給pod上label用法的affinity.

## resource_requriement

可以要求pod要多少資源,這樣scheduler規劃node時就會適當分配！你可以規定基本資源需求量,以及資源上限,cpu及記憶體都會受到規範.
當一個 Pod 超出其記憶體限制時，Kubernetes 會以 "OOMKilled" 的原因（Out of Memory）終止該 Pod。當記憶體使用量失控時，一個 Pod 可能會變得無響應甚至崩潰。如果一個 Pod 超出其 CPU 限制，Kubernetes 則會簡單地限制該 Pod 的 CPU 使用,這會讓該pod性能降低.

預設k8s給每個pod資源是0.5cpu跟256Mi記憶體,如果node目前資源不符合pod基本需求,會發現pod狀態處在pending state.

調整pod資源設定檔.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
 containers:
 - name: simple-webapp-color
   image: simple-webapp-color
   ports:
    - containerPort:  8080
   resources:
     requests:
      memory: "1Gi"
      cpu: "1"
     limits:
       memory: "2Gi"
       cpu: "2"
```

你也可以限制namespace使用的資源量

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: example-resourcequota
  namespace: your-namespace  # 替換為你想要應用此資源配額的命名空間
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    pods: "10"

```

## deamon_set

DaemonSets 的目的是確保在集群中的每個節點上都運行一個 Pod 的副本，因此不需要像一般的 Pod 一樣由 kube-scheduler 進行調度,而是透過api server去管理daemonset contoller.

應用示範:確保在每個節點上運行一個具有特定標籤的 Fluentd-elasticsearch 容器，做到日誌收集以及轉發！Fluentd後面可以接elasticsearch或者s3等其他對象, 除此之外像是CNI的實現也是透過deamon_set去做到！

```yaml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - image: registry.k8s.io/fluentd-elasticsearch:1.20
        name: fluentd-elasticsearch
```

## k8s scheduling

一般pod都是透過kubescheduler去規劃要放在哪個node,沒有起scheduler你要自己分配pod到node.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  nodeName: your-node-name
  containers:
  - name: my-container
    image: nginx

```

## k8s cheatsheet
```
Create an NGINX Pod
kubectl run nginx --image=nginx
Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml
Deployment
Create a deployment
kubectl create deployment --image=nginx nginx
Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
Generate Deployment with 4 Replicas
kubectl create deployment nginx --image=nginx --replicas=4
You can also scale a deployment using the kubectl scale command.
kubectl scale deployment nginx --replicas=4
Another way to do this is to save the YAML definition to a file and modify
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
You can then update the YAML file with the replicas or any other field before creating the deployment.
Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors)
Or
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml
kubectl run redis -l tier=db --image=redis:alpine
kubectl expose pod redis --port=6379 --name redis-service
kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
kubectl run custom-nginx --image=nginx --port=8080
kubectl create namespace dev-ns
kubectl run httpd --image=httpd:alpine --port=80 --expose
kubectl expose pod httpd --port=80 --name=httpd


```

## static pod in k8s

起pod有兩個方式

- 透過 API Server
- 使用 Kubelet 建立static pod

Static Pods 的主要用途是運行控制平面組件，如 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 等。這些組件需要在 Kubernetes 環境啟動時提前運行，因此通常使用 Static Pods 來實現。

Kubelet 可以管理靜態 Pod，這些 Pod 的配置文件通常存儲在 /etc/kubernetes/manifests 目錄中。它會讀取這些配置文件，並運行相應的 Pod。如果配置文件發生更改，Kubelet 將重新創建相關的 Pod，確保它們保持運行。

kube scheduler不管static pod(包含control plane components 跟 daemonsets(logging agent and monitoring agent)這些!

如何查詢staticPodPath

1. ps -aux | grep kubelet.  然後去看設定檔位置ex. --config=/var/lib/kubelet/config.yaml.
2. 接著去看該設定檔`/var/lib/kubelet/config.yaml` 寫的是staticPodPath路徑,往那邊查就會看到kubelet管理的pod的yaml,只要有改動他就會重啟

## 環境變數

k8s可以用多種方法去設定(plain key value, ConfigMap, Secrets)環境變數.

### plain key value

  ```
  apiVersion: v1
  kind: Pod
  metadata:
    name: simple-webapp-color
  spec:
   containers:
   - name: simple-webapp-color
     image: simple-webapp-color
     ports:
     - containerPort: 8080
     env:
     - name: APP_COLOR
       value: pink
  ```

### ConfigMap

建立ConfigMap方法主要分為imperative跟declarative兩種

- imperative方式
  
  ```bash
    kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod
    kubectl create configmap app-config --from-file=app_config.properties #(Another way)
  ```

- declarative方式
  - 直接由yaml定義
  
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
    name: app-config
    data:
    APP_COLOR: blue
    APP_MODE: prod
    ```

  - 由file定義,`kubectl create configmap game-config --from-file=`

    ```text  title="game.properties"
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30
    ```

建立ConfigMaps之後要inject到pod裡面作為環境變數,ingect方式有三種.

- 直接整份ConfigMap匯入
  
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: simple-webapp-color
  spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    envFrom:
    - configMapRef:
        name: app-config
  ```

- 匯入ConfigMap其中一個key

```yaml
  env:
    - name: APP_COLOR
      valueFrom:
        name: app-config
        key: APP_COLOR
```

- 以檔案掛載方式匯入

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: dapi-test-pod
  spec:
    containers:
      - name: test-container
        image: registry.k8s.io/busybox
        command: [ "/bin/sh", "-c", "ls /etc/config/" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config
    volumes:
      - name: config-volume
        configMap:
          # Provide the name of the ConfigMap containing the files you want
          # to add to the container
          name: special-config
    restartPolicy: Never
  ```

### Secrets

跟ConfigMap很像差別是在所帶入的kind不同,且值會需要先經過base64編碼,這邊放兩者差別,

這樣會被看光光

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  DB_Host: mysql
  DB_User: root
  DB_Password: paswrd
```

```bash
# 稍微擋一下
  $ echo -n "mysql" | base64
  $ echo -n "root" | base64
  $ echo -n "paswrd"| base64
```

帶入得到secret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bX1zcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk
```

- To decode secrets
  
```bash
#解碼還原
$ echo -n "bX1zcWw=" | base64 --decode
$ echo -n "cm9vdA==" | base64 --decode
$ echo -n "cGFzd3Jk" | base64 --decode
```

帶入Secrets到Pod內

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bX1zcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk
```

```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: simple-webapp-color
  spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    envFrom:
    - secretRef:
        name: app-secret
  ```

跟configmap相通也可以帶入configmap中的某個key,或者用file方式mount到Pod內部！

## multiconatiner in pod

也就是一個pod裡面有多container,使用情境有:

- app的log透過filebeat這類日誌轉發器到ELK

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  containers:
  - image: kodekloud/filebeat-configured
    name: sidecar
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/event-simulator/ # container的資料夾位置
  - image: kodekloud/event-simulator
    name: app
    volumeMounts:
    - name: log-volume
      mountPath: /log/ #contianer裡的位置
  volumes:
    - name: log-volume
      hostPath:
        path: /var/log/webapp # 主機上位置
        type: DirectoryOrCreate

```

- container內部做git clone原始碼(透過initContainers方式)

```yaml title=""
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  initContainers:
    - name: git-clone-init-container
      image: alpine/git
      args:
        - clone
        - https://github.com/your-repo/your-app.git
        - /app
      volumeMounts:
        - name: app-volume
          mountPath: /app
  containers:
    - name: main-container
      image: your-web-app-image
      volumeMounts:
        - name: app-volume
          mountPath: /app
  volumes:
    - name: app-volume
      emptyDir: {}

```

## k8s version upgrade

v1.11.3 major features fix bugs, major components can be different version. none of component can be at a version higher than the kube api server,  rest can be minus 1 version

kubeadm upgrade plan(go one minor version at a time)
first upgrade master node v1.11
upgrade woker node(v1.10 -> v1.11)

0- kubeadm upgrade plan
kubectl drain controlplane  --ignore-daemonsets
kubectl get nodes (check shcedule disabled)
kubectl drain node-1–ignore-daemonsets
apt update
apt-get install kubeadm=1.27.0-00
kubeadm upgrade apply v1.27.0
apt-get install kubelet=1.27.0-00
systemctl daemon-reload
systemctl restart kubelet

確認ok,把kubectl uncordon node-1
依序把node做完
!!! 一個一個做 不然會爆掉 尤其是在drain部分！

## backup and restore

有哪些要備份的東東？

1. resource config( 就你的個元件設定檔, this is why declartive way is important)
kubectl get all -A -o yaml > all-deploy.yml 也有其他工具幫忙備份
2. etcd cluster的db資料備份
3. persistent volumes(一般的硬碟部分)

etcd還原過程就是把備份的db或硬碟掛載到指定資料夾底下,然後用指令還原db

```bash
ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup snapshot restore /opt/snapshot-pre-boot.db
```

之後到static pod設定檔( ` /etc/kubernetes/manifests ` 底下)那邊修改掛載的db路徑, 一但設定檔改變之後kublet會自動重啟.可以用`watch "crictl ps | grep etcd" .`確認ETCD重啟過程,如果重啟失敗一直卡在那,可以用`kubectl delete pod -n kube-system etcd-controlplane`把東西砍掉讓他自己在重啟一次.基本上etcd掛了其他東西也會掛.

!! 有關etcd掛掉的環元！
超超重要 東西restroe一定要注意權限歸屬問題 權限錯誤東西寫不進去
chown -R etcd:etcd /var/lib/etcd-data-new

## cluster upgrade

 From master node, run 'kubectl drain' command to move the workloads to other nodes

  ```
  kubectl drain node-1
  ```

- Upgrade kubeadm and kubelet packages

  ```
  apt-get upgrade -y kubeadm=1.12.0-00
  apt-get upgrade -y kubelet=1.12.0-00
  ```

- Update the node configuration for the new kubelet version

  ```
  kubeadm upgrade node config --kubelet-version v1.12.0
  ```

- Restart the kubelet service

  ```
  systemctl restart kubelet
  ```

- Mark the node back to schedulable

  ```
  kubectl uncordon node-1
  ```

對每個node作以上操作.

## TLS in k8s

產admin key
用admin key產出admin csr(確保這個csr是你產出)
把admin csr給予CA簽署(這邊會用到ca.crt跟ca.key,ca.key用來簽署用,ca.crt是給收到admin.crt的使用者去驗證是ca簽署）並產出admin.crt

1. 生成 CA (Certificate Authority) 證書和金鑰：

- 生成 CA 金鑰 ca.key, `openssl genrsa -out ca.key 2048`
- 創建 CA 資格請求 ca.csr，但不要簽署它。您需要使用 ca.key 來簽署它, `openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr`。
- 使用 ca.key 簽署 CA 資格請求，生成自簽名的 CA 證書 ca.crt,`openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt`。這個證書將用於驗證其他證書。
  
1. 生成 admin 用戶證書和金鑰：

- 生成 admin 用戶金鑰 admin.key`openssl genrsa -out admin.key 2048`。
- 創建 admin 用戶資格請求 admin.csr。確保此請求由您生成,`openssl req -new -key admin.key -subj "/CN=kube-admin" -out admin.csr`。
- 使用 CA 證書 ca.crt 和 CA 金鑰 ca.key 簽署 admin 用戶資格請求，生成 admin.crt,`openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt`。
  
3. 使用相同的過程來生成其他組件（scheduler、controller manager、replicaset、daemonset、kubeproxy、apiserver-kublet、apiserver-etcd、kubelet-client）的證書和金鑰。

## log

crictl ps -a
crictl logs containe-ID
crictl ps -a 是一個用於容器運行時（Container Runtime）管理的命令，它通常與 CRI（Container Runtime Interface）兼容的容器運行時一起使用。CRI 是 Kubernetes 中用於與容器運行時（如 Docker、containerd、CRI-O 等）進行通信的標準接口。

另外就像是docker可以看docker logs,k8s也可以,放在/var/lib/pods/podid上
以上就是兩種排錯control plane 方法

## Authorization

k8s使用者類型分兩種, user以及service account,例如admin  user創建deployment,service等,而service account則像是prometheus從api server拉metric,或者jenkins部署程式到node.
使用者及程式對k8s的操作需要先經過api-server做驗證,驗證機制有很多種,另外也須敘述授權使用者能執行的權限！

1. static password file
2. static token file
3. certificates
4. identity services

這邊用certificate作為示範設定

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: development
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: kubernetes-on-aws
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: production
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: test-cluster-1
contexts:
- context:
    cluster: kubernetes-on-aws
    user: aws-user
  name: aws-user@kubernetes-on-aws
- context:
    cluster: test-cluster-1
    user: dev-user
  name: research
- context:
    cluster: development
    user: test-user
  name: test-user@development
- context:
    cluster: production
    user: test-user
  name: test-user@production
current-context: research
kind: Config
preferences: {}
users:
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
```
clusters：這個部分列出了不同的 Kubernetes 集群配置，每個集群配置包含以下信息：

- name：集群的名稱，用於識別不同的集群。
- cluster.certificate-authority：用於驗證伺服器證書的證書授權機構 (CA) 文件的路徑。
- cluster.server：Kubernetes 控制平面的 API 伺服器的 URL。

contexts：這個部分列出了不同的使用情境（context），每個情境包含以下信息：

- name：情境的名稱，用於識別不同的情境。
- context.cluster：指定該情境所關聯的集群。
- context.user：指定該情境使用的使用者（user）。

current-context：指定當前活動情境的名稱，也就是當你運行 kubectl 命令時，它將使用的上下文。

preferences：用於存儲配置文件的個人首選選項，目前是空的。

users：這個部分列出了不同的使用者配置，每個使用者配置包含以下信息：

- name：使用者的名稱，用於識別不同的使用者。
- user.client-certificate：指定用戶的客戶端證書文件的路徑。
- user.client-key：指定用戶的客戶端密鑰文件的路徑。

主要就是將user對應到cluster內.接下來會講到授權操作.

## Authorization Mechanisms

1. Node: 因為kublet需要對API server下達 Read services, endpoints, nodes, pods,並 Write node status,pod status,events,所以他也需要api server授予權限
2. ABAC（Attribute-Based Access Control）:分別對各個使用者授予操作權限
3. RBAC（Role-Based Access Control）: 先建立能操作cluster的角色,在授予個使用者角色.
4. Webhook: 透過第三方插件向api server要求授權.
   
在role部分會區分為namespace裡面有操作權限的role,以及對於cluster有操作權限(通常用於賦予對集群範圍資源的訪問權限，例如 Nodes、Namespaces、PersistentVolumes 等)的ClusterRole.


以下為role建立,先建立role,並將role連結到user上.
```yaml
# RBAC Role: developer
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["list", "get", "create", "update", "delete"]
    resourceNames: ["blue", "orange"]

# RBAC RoleBinding for dev-user
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-user-developer-binding
subjects:
  - kind: User
    name: dev-user
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io

```
role這邊排查有點麻煩 他完全依據namespace切分期使同role也不會列出在其他namespace的權限,排查順序

1. 先看rolebindings資訊最多(先get 在針對specific做describe)
2. 看role 這邊-n要分好
3. 可以用kubectl edit role rolename -n去做修改role
role要用權限授權的 kube/config那個

ClusterRole部分跟role很像,也是先建立cluster role,再把cluster role透過binding連結到user.
```
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole 
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
```

## service accounts
透過程式去跟k8s api server的api互動並取得資源,建立service account拿到token,拿token去授權操作,注意token有效期限.

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-sa
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: dashboard-sa # Name is case sensitive
  namespace: default
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - watch
  - list
```

因為在v1.24版之後,要取得token會用指令打tokenserver,ex.`kubectl create token dashboard-sa`,或者你也可以直接把

如何使用它？

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2023-10-20T07:21:56Z"
  generation: 1
  name: web-dashboard
  namespace: default
spec:
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa # 這邊要註記
      containers:
      - env:
        - name: PYTHONUNBUFFERED
          value: "1"
        image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
```

解碼jwt token,取裡面意思
```bash
jq -R 'split(".") | select(length > 0) | .[0], .[1] | @base64d | fromjson' <<<
```

## k8s登入私有image庫拉image

建立登入token
```bash
kubectl create secret docker-registry private-reg-cred --docker-server=myprivateregistry.com:5000 --docker-username=dock_user --docker-password=dock_password --docker-email dock_user@myprivateregistry.com
```

在spec要建立pod或deployment時要這樣寫

```yaml
spec:
  containers:
  - image: myprivateregistry.com:5000/nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
  dnsPolicy: ClusterFirst
  imagePullSecrets:
  - name: private-reg-cred
```

## docker sercurity
 他會預設限制網路SYS_ADMIN,MAC_ADMIN,BRODCAST, ….. 可以用--cap-drop, –priveliged 權限全開.k8s也可以修改使用者.
 
 ```yaml
 ---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  securityContext: # 修改啟動使用者
    runAsUser: 1010
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
 ```

## k8s 內部網路
cluster內部預設是網路全通,如果要限定pod的網路進出可以用NetworkPolicy.

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```


## storage
k8s底層用CSI管控k8s volume,只要開發的硬碟有符合相關格式,就可以在k8s使用,因為資料沒有透過掛載硬碟,一但pod掛掉資料也會丟失,所以要透過掛載volume跟volumeMounts去保存資料！但是在多節點情況下,不建議這樣mount.

### 一般volume跟volumeMount
以下是給單node去做掛載所使用的,在不同node執行時會遇到沒有資料的問題,因此會建議用`Persistent Volumes`解決方案. 
```yaml title="volumeMountDemo.yaml"
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: myimage
    volumeMounts:
    - mountPath: "/data"  # 容器內掛載的路徑
      name: myvolume       # 對應到 Volume 的名稱
  volumes:
  - name: myvolume         # Volume 的名稱，用於對應到 VolumeMounts
    hostPath:
      path: /mnt/data      # 實際的主機路徑
```
### Persistent Volumes
Persistent Volumes 概念是建立在以cluster為單位的硬體儲存方案,所以可以做到讓不同node上面的pod可以使用共同的volume. k8s為了讓各種volume類型(多種方案)抽象化,所以引入Persistent Volume Claims(PVC)觀念去匹配符合的volume, pod會面PVC選取硬碟.

Persistent Volume是cluster層級,Persistent Volume Claim是namespace層級.
#### Create the Persistent Volume
建立Persistent Volume.

```
pv-definition.yaml

kind: PersistentVolume
apiVersion: v1
metadata:
    name: pv-vol1
spec:
    accessModes: [ "ReadWriteOnce" ]
    capacity:
     storage: 1Gi
    hostPath:
     path: /tmp/data
```
```
$ kubectl create -f pv-definition.yaml

```

#### Create the Persistent Volume Claim
建立Persistent Volume Claim名為myclaim
```
pvc-definition.yaml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes: [ "ReadWriteOnce" ]
  resources:
   requests:
     storage: 1Gi
```
```
$ kubectl create -f pvc-definition.yaml
```

#### Create a Pod
使用前面建立好名myclaim為的Persistent Volume Claim,並將其掛載到/var/www/html這個路徑
```
pod-definition.yaml

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: web
  volumes:
    - name: web
      persistentVolumeClaim:
        claimName: myclaim
```
```
$ kubectl create -f pod-definition.yaml

```

### storage class
雲端部分提供了一個動態的硬碟機制,這是相較於之前都需要先建立Persistent Volume再建立Persistent Volume Claim,可以透過
StorageClass去做成一個大的硬碟池,之後只要向這個硬碟池做Persistent Volume Claim即可.

#### Dynamic Provisioning

```
sc-definition.yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: google-storage
provisioner: kubernetes.io/gce-pd
```

#### Create a Storage Class
create storage class
```
$ kubectl create -f sc-definition.yaml
storageclass.storage.k8s.io/google-storage created
```

#### List the Storage Class

```
$ kubectl get sc
NAME             PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
google-storage   kubernetes.io/gce-pd   Delete          Immediate           false                  20s
```

#### Create a Persistent Volume Claim

```
pvc-definition.yaml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: google-storage       
  resources:
   requests:
     storage: 500Mi
```
```
$ kubectl create -f pvc-definition.yaml

```
#### Create a Pod

```
pod-definition.yaml

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: frontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: web
  volumes:
    - name: web
      persistentVolumeClaim:
        claimName: myclaim
```

```
$ kubectl create -f pod-definition.yaml
```

## Networking
docker 的網路會有三種模式, bridge , host, none, bridge模式會創建橋接介面(類似虛擬化交換器),並給予該介面一個ip,能夠檢查和轉發來自不同裝置的數據幀（frame),讓容器間可以彼此溝通,如果你使用docker-compose起容器,可以觀察他會另外建立一個新的橋接介面,而host模式會共用主機網路,基本上就是把自己當成宿主機,那none模式就是建立一個不連外的獨立網路空間,以上可以用`docker network ls`跟`ip address`,`route`等指令去觀察結果.

而k8s部分,本身主要管理容器生命週期,容器間網路,儲存體這些都會使用公開見面讓其他人開發.網路規劃部分由CNI負責,CNI要做到 1. pods間彼此不透過NAT跟其他pods溝通, 2. node上的agent(e.g. system daemons, kubelet)需要跟node上其他pods溝通.

Kubernetes的網路配置大致如下：當節點加入集群時，會根據--advertise-address參數向API伺服器註冊自己的IP位址，同時在本地建立網路介面並配置IP範圍以進行通訊。這代表著每個節點會有不同的IP位址，這些IP位址位於相同的子網段。接著，如果使用CNI插件（例如Weave），它會在每個節點上建立一個DaemonSet。DaemonSet中的容器會自動執行腳本以建立虛擬交換器，並透過覆蓋overlay技術讓Pod在不同節點上建立網路路由，使得Pod能夠位於相同的IP位址範圍，例如10.244.0.0/16，並且能夠直接通訊而無需NAT。

所以,基本上會建立多個network interface在k8s集群的各個node內部.

- 查詢有哪些切割出來的獨立網路空間, `ip netns`
- 查詢node上面有哪些network interfaces, `ip link`.
- 查詢node上面network interface的ip, `ip addr`.
- 查詢node上面路由表, `route`.
- dns設定解析指向的機器,`cat /etc/resolv.conf`

### CNI network plugins
Flannel, Calico,Weave...有很多插件建立在CNI的要求上.

以weave為例,`https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml` 當執行`kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml` 會創建role跟
rolebinding,起container會先用weave-init(initContainers)去執行init.sh確認相關默認位置有資料夾存在,並安裝插件,而container部分分別有weave跟weave-npc兩個,前者啟動會執行lunch.sh用以配置容器的網路設定.

## service種類
[參考網址](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types),共有四種type.

1. ClusterIP: 將deployment或pod這類容器給予一個cluster級別IP,用於集群內個service溝通.
2. NodePort：這會用在測試或者想簡單對外的時候,會在node上開一個port然後把該port映射到k8s的pod在用的虛擬交換器,再到對應的pod上面的port.
3. LoadBalancer:把service對外開放的其中一種方式,需要自己建立load balancer,或者用雲端平台的.
4. ExternalName:把外部dns的例如db.example.com這個db,可以透過創建內部服務,例如:my-service,然後將他對應到該hostname.集群內部其他應用程式可以透過`my-service`來連接到該資料庫.

### ClusterIp對外
1. Ingress:一種將外部的 HTTP 和 HTTPS 路由映射到集群內部服務的機制。想成第七層反向代理,類似nginx.
   
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: bar.foo.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service2
            port:
              number: 80
```
2. Gateway:Gateway通常用於代表Service Mesh（服務網格），例如 Istio、Linkerd等。它們主要過用是設計用於管理外部客戶端到群集內部服務的流量,有點像是ingress升級版,多了管理流量等功能.
2. LoadBalancer:透過lb把流量引入內部service.

## K8s Reference Docs

- <https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#create-configmaps-from-files>


